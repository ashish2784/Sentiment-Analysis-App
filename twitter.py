# -*- coding: utf-8 -*-
"""Twitter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Orv0PiG1hNeVithrlceq9vyxS9wc2poZ

First, we load the dataset and perform some initial exploration to understand its structure and content.
"""

import pandas as pd

# Load dataset
df = pd.read_csv("twitter_training.csv", header=None)

# Rename columns
df.columns = ["id", "entity", "sentiment", "text"]

# Basic info
print(df.shape)

print(df["sentiment"].value_counts())

"""Let's look at the first few rows of the dataframe to get a sense of the data."""

print(df.head())

"""We define and apply a function to clean the text data. This function handles lowercasing, removes URLs, mentions, hashtags, and punctuation, and keeps only letters and spaces."""

import re, string

def clean_text(text):
    text = text.lower()                                    # lowercase
    text = re.sub(r"http\S+|www\S+", "", text)             # remove URLs
    text = re.sub(r"@\w+|#\w+", "", text)                  # remove mentions/hashtags
    text = re.sub(r"[^a-zA-Z\s]", "", text)                # keep only letters
    text = text.translate(str.maketrans("", "", string.punctuation))  # remove punctuation
    return text.strip()

# Apply cleaning
df["clean_text"] = df["text"].astype(str).apply(clean_text)

print(df[["text", "clean_text"]].head(10))

"""We split the data into training and testing sets and then use TF-IDF vectorization to convert the text data into numerical features."""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Train-test split (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(
    df["clean_text"], df["sentiment"],
    test_size=0.2, random_state=42, stratify=df["sentiment"]
)

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print("Train shape:", X_train_tfidf.shape)
print("Test shape:", X_test_tfidf.shape)

"""We train a Multinomial Naive Bayes model and evaluate its performance."""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Train model
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

# Predictions
y_pred = model.predict(X_test_tfidf)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

""" train a Logistic Regression model and evaluate its performance.

"""

from sklearn.linear_model import LogisticRegression

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=10000, n_jobs=-1)
log_reg.fit(X_train_tfidf, y_train)

# Predictions
y_pred_lr = log_reg.predict(X_test_tfidf)

# Evaluation
print("Accuracy (Logistic Regression):", accuracy_score(y_test, y_pred_lr))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))

""" visualize the confusion matrix for the Logistic Regression model.

"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Confusion Matrix for Logistic Regression
cm = confusion_matrix(y_test, y_pred_lr, labels=log_reg.classes_)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=log_reg.classes_, yticklabels=log_reg.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

"""
check the distribution of predictions for the Logistic Regression model.
"""

import pandas as pd

pred_dist = pd.Series(y_pred_lr).value_counts()
print("Prediction distribution:\n", pred_dist)

"""improve accuacruy using SVM

Install necessary libraries
"""

!pip install emoji contractions

"""Now that the necessary libraries are installed, I can proceed with the updated text cleaning function that includes handling contractions and emojis, and then we can explore using SVM for improved accuracy."""

import re, emoji, contractions
from nltk.corpus import stopwords
import nltk
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = contractions.fix(text)                  # expand contractions
    text = emoji.demojize(text)                    # ðŸ™‚ -> ":slightly_smiling_face:"
    text = re.sub(r"http\S+|www\S+", "", text)     # remove URLs
    text = re.sub(r"@\w+|#\w+", "", text)          # remove mentions & hashtags
    text = re.sub(r"[^a-zA-Z\s]", "", text)        # remove special chars & numbers
    text = " ".join([w for w in text.split() if w not in stop_words])  # remove stopwords
    return text.strip()

# Apply cleaning
df["clean_text"] = df["text"].astype(str).apply(clean_text)

print(df[["text", "clean_text"]].head(10))

"""Now that the necessary libraries are installed, I can proceed with the updated text cleaning function that includes handling contractions and emojis, and then we can explore using SVM for improved accuracy."""

import re, emoji, contractions
from nltk.corpus import stopwords
import nltk
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = contractions.fix(text)                  # expand contractions
    text = emoji.demojize(text)                    # ðŸ™‚ -> ":slightly_smiling_face:"
    text = re.sub(r"http\S+|www\S+", "", text)     # remove URLs
    text = re.sub(r"@\w+|#\w+", "", text)          # remove mentions & hashtags
    text = re.sub(r"[^a-zA-Z\s]", "", text)        # remove special chars & numbers
    text = " ".join([w for w in text.split() if w not in stop_words])  # remove stopwords
    return text.strip()

"""We apply the improved cleaning function to the text data.


"""

df["clean_text"] = df["text"].astype(str).apply(clean_text)

""" define an improved text cleaning function that includes handling contractions, emojis, and stopwords."""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=10000, stop_words="english", ngram_range=(1,2))
X = vectorizer.fit_transform(df["clean_text"])
y = df["sentiment"]

"""Split the data and train a Linear SVC model (SVM) for sentiment classification and evaluate its performance."""

from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

model = LinearSVC()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Perform a grid search to find the best hyperparameter for the Linear SVC model."""

from sklearn.model_selection import GridSearchCV

param_grid = {"C": [0.01, 0.1, 1, 10]}
grid = GridSearchCV(LinearSVC(), param_grid, cv=3, scoring="accuracy", n_jobs=-1)
grid.fit(X_train, y_train)

print("Best C:", grid.best_params_)
print("Best accuracy:", grid.best_score_)

"""Re-apply the improved text cleaning function to ensure it's using the correct NLTK data."""

import re, emoji, contractions
from nltk.corpus import stopwords
import nltk

# Download stopwords if not already
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = str(text).lower()
    text = contractions.fix(text)                  # expand contractions: can't -> cannot
    text = emoji.demojize(text)                    # ðŸ™‚ -> ":slightly_smiling_face:"
    text = re.sub(r"http\S+|www\S+", "", text)     # remove URLs
    text = re.sub(r"@\w+|#\w+", "", text)          # remove mentions & hashtags
    text = re.sub(r"[^a-zA-Z\s]", "", text)        # keep only letters
    text = " ".join([w for w in text.split() if w not in stop_words])  # remove stopwords
    return text.strip()

# Apply to your dataset
df["clean_text"] = df["text"].apply(clean_text)

print(df[["text", "clean_text"]].head(10))

"""use TF-IDF vectorization with unigrams and bigrams and a higher max_features for potentially better performance."""

from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF with unigrams + bigrams
vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=20000)

X = vectorizer.fit_transform(df["clean_text"])
y = df["sentiment"]

print("TF-IDF shape:", X.shape)

"""Train a Logistic Regression model with updated parameters and evaluate its performance."""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Logistic Regression with stronger regularization
model = LogisticRegression(max_iter=400, C=2.0, class_weight='balanced')
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

"""SVM

Train a Linear SVC model (SVM) and evaluate its performance.
"""

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

# Instantiate and train the model
model = LinearSVC()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""

Visualize the confusion matrix for the SVM model.

"""

# Confusion Matrix for SVM
cm_svm = confusion_matrix(y_test, y_pred, labels=model.classes_)

plt.figure(figsize=(8,6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - SVM")
plt.show()

"""Compare the accuracy of the Logistic Regression and SVM models using a bar chart."""

import matplotlib.pyplot as plt

# Store model accuracies
accuracies = {
    "Logistic Regression": accuracy_score(y_test, y_pred_lr),
    "SVM": accuracy_score(y_test, y_pred)
}

# Create bar chart
models = list(accuracies.keys())
accuracy_values = list(accuracies.values())

plt.figure(figsize=(8, 6))
plt.bar(models, accuracy_values, color=["blue", "green"])
plt.ylim(0, 1)  # Accuracy is between 0 and 1
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison")
plt.show()

"""Notebook focuses on performing sentiment analysis on a Twitter dataset to classify tweets into one of four sentiment categories: Negative, Positive, Neutral, and Irrelevant. The process involved:

1.  **Data Loading and Exploration:** Loading the dataset and examining its basic structure and the distribution of sentiment labels.
2.  **Text Preprocessing:** Implementing a comprehensive text cleaning pipeline that includes handling contractions and emojis, removing noise like URLs and mentions, and removing common English stopwords to prepare the text data for model training.
3.  **Feature Extraction:** Converting the cleaned text data into numerical features using TF-IDF vectorization, considering both unigrams and bigrams to capture more context.
4.  **Model Training and Evaluation:** Training two different machine learning models, Logistic Regression and Linear Support Vector Machine (SVM), on the processed data. The models were evaluated using standard metrics like accuracy and classification reports.
5.  **Performance Comparison:** Visualizing the performance of the models using confusion matrices and a bar chart comparing their overall accuracy.

The results showed that the Linear SVM model achieved a higher accuracy (approximately 84.2%) compared to the Logistic Regression model (approximately 80.9%) on this dataset, indicating its better performance in classifying the sentiment of the tweets.
"""